# -*- coding: utf-8 -*-

import time
import os
from subprocess import Popen, PIPE
from urllib.parse import urlparse


__otg__ = "OTG-INFO-003"

class Info_003():
    

    def __init__(self, url):
        self.url = url
        self.ruta = os.getcwd()+"/scrips"
        self.nombre =  "robots.txt"
        self.resultado = ""
        self.sinResultado = "No ha sido posible encontrar el archivo robots.txt en la raíz de la aplicación"
        self.recomendaciones = ""
    
    
    def test_robots(self,url):
        
        stdout = Popen("wget -q %s%s %s" % (url,"robots.txt","-O " + self.ruta +"/"+ self.nombre), shell=True, stdout=PIPE).stdout
        time.sleep(10)

    def check_file(self):        

        if os.stat(self.ruta +"/"+  self.nombre).st_size == 0:
            self.resultado = self.sinResultado
        else:
            self.read_robots()


    def read_robots(self):
        self.read_robotsres = ""
        with open (self.ruta + "/"+  self.nombre,"r") as file:
            for line in file:
                self.resultado += line
        file.close()

    def split_url(self,url):
        url_tmp = urlparse(url)
        u = url_tmp.scheme + "://" + url_tmp.netloc
        return u


    
    def verify_url(self,url):
        if url[-1] != "/":
            url += "/"
            return url
        else:
            return url  


    
    def delete_file(self):
        os.system("rm -r " + self.ruta + "/" + self.nombre)

    
    def main(self):
        
        print("================ Iniciando prueba OTG-INFO-003 ==================\n")
        print("Detectando archivos para fuga de información ...\n")
        url_split  = self.split_url(self.url)
        url = self.verify_url(url_split)
        self.test_robots(url)
        self.check_file()
        self.delete_file()
        
     
    
        if self.resultado != self.sinResultado:
            self.resultado = "Se obtuvo el archivo robots.txt en el directorio raíz de la aplicación." 
            
            self.recomendaciones = "Existen otros “robots maliciosos” que suelen buscar direcciones de correos o formularios para hacer SPAM, que podrán acceder a los lugares que se haya prohibido si lo desean. Para bloquear estos otros robots, se debe ingresar al fichero “.htaccess.” "
            self.recomendaciones = "El archivo es público, lo que significa que cualquiera puede verlo con sólo teclear " + str(url)+ "robots.txt, por tanto, si se utiliza archivos robots.txt para esconder información privada, se debe utilizar un método más seguro como la protección con contraseña." 
            
        print("================ Terminando prueba OTG-INFO-003 ==================\n")
        return self.resultado, self.recomendaciones










"""url="https://www.google.com"  #TODO: SOLO URL
otg_003 =info_003(url)
otg_003.main()"""